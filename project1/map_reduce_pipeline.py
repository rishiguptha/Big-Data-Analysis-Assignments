import os
import sys
import glob
from collections import defaultdict
import argparse

# --- Professional Configuration ---
# NetID should be the only thing a user needs to change.
NET_ID = "rmankala"

# Base directory for all project-related output files.
# Using os.path.expanduser('~') could also work but this is more explicit for HPC.
BASE_PROJECT_DIR = f"/gpfs/projects/AMS598/class2025/{NET_ID}"

# Static location for the input data as specified in the project description.
INPUT_DATA_DIR = "/gpfs/projects/AMS598/projects2025_data/project1_data/"

# Directory for all temporary files generated by mappers and reducers.
INTERMEDIATE_DIR = os.path.join(BASE_PROJECT_DIR, "intermediate")

# The final, single output file, as requested.
FINAL_OUTPUT_FILE = os.path.join(BASE_PROJECT_DIR, "output.txt")

# Define the degree of parallelism.
NUM_MAPPERS = 4
NUM_REDUCERS = 4


def setup_directories():
    """
    Ensures that the directory for intermediate files exists.
    This prevents file-not-found errors when the script tries to write.
    """
    try:
        os.makedirs(INTERMEDIATE_DIR, exist_ok=True)
    except OSError as e:
        sys.exit(f"Error: Could not create intermediate directory at {INTERMEDIATE_DIR}. Reason: {e}")


def run_mapper(mapper_id):
    """
    Mapper Task:
    1.  Calculates its assigned chunk of the 16 input data files.
    2.  Reads each assigned file line by line.
    3.  Counts the occurrences of each integer.
    4.  Writes the partial counts to a unique intermediate file on disk.
    
    Args:
        mapper_id (int): The unique ID for this mapper process (e.g., 0, 1, 2, or 3).
                         This is provided by the Slurm job array task ID.
    """
    print(f"Mapper [{mapper_id}]: Starting.")
    
    all_files = sorted(glob.glob(os.path.join(INPUT_DATA_DIR, "*.txt")))
    if not all_files:
        sys.exit(f"Error: No input files found at {INPUT_DATA_DIR}. Please check the path.")
    
    # Simple and effective way to distribute files among mappers.
    files_per_mapper = len(all_files) // NUM_MAPPERS
    start_index = mapper_id * files_per_mapper
    end_index = start_index + files_per_mapper
    
    # The last mapper takes any remaining files if the division isn't perfect.
    if mapper_id == NUM_MAPPERS - 1:
        end_index = len(all_files)

    assigned_files = all_files[start_index:end_index]
    print(f"Mapper [{mapper_id}]: Assigned {len(assigned_files)} files.")

    local_counts = defaultdict(int)

    for filepath in assigned_files:
        with open(filepath, 'r') as f:
            for line in f:
                try:
                    num = int(line.strip())
                    local_counts[num] += 1
                except (ValueError, TypeError):
                    # Gracefully skip lines that are not valid integers.
                    continue
    
    # Write results to a unique file in the intermediate directory.
    intermediate_filepath = os.path.join(INTERMEDIATE_DIR, f"map_output_{mapper_id}.txt")
    with open(intermediate_filepath, 'w') as f:
        for number, count in local_counts.items():
            f.write(f"{number}\t{count}\n")
            
    print(f"Mapper [{mapper_id}]: Finished. Results saved to {intermediate_filepath}")


def run_reducer(reducer_id):
    """
    Reducer Task:
    1.  Scans all intermediate files produced by the mappers.
    2.  Uses a hashing function (modulo) to determine which numbers it is responsible for.
    3.  Aggregates the counts for its assigned numbers from all mapper files.
    4.  Writes its final aggregated counts to a unique file in the same intermediate directory.

    Args:
        reducer_id (int): The unique ID for this reducer process (e.g., 0, 1, 2, or 3).
                          This is provided by the Slurm job array task ID.
    """
    print(f"Reducer [{reducer_id}]: Starting.")
    
    final_counts = defaultdict(int)
    
    # A reducer must read the output from ALL mappers.
    mapper_files = glob.glob(os.path.join(INTERMEDIATE_DIR, "map_output_*.txt"))
    if not mapper_files:
        sys.exit("Error: No mapper output files found. Did the map phase run correctly?")

    for filepath in mapper_files:
        with open(filepath, 'r') as f:
            for line in f:
                try:
                    parts = line.strip().split('\t')
                    if len(parts) == 2:
                        number, count = int(parts[0]), int(parts[1])
                        
                        # Hashing function: Assigns this number to a specific reducer.
                        if number % NUM_REDUCERS == reducer_id:
                            final_counts[number] += count
                except (ValueError, TypeError):
                    continue

    # Write final results to a unique file in the intermediate directory.
    output_filepath = os.path.join(INTERMEDIATE_DIR, f"reduce_output_{reducer_id}.txt")
    with open(output_filepath, 'w') as f:
        for number, count in final_counts.items():
            f.write(f"{number}\t{count}\n")
            
    print(f"Reducer [{reducer_id}]: Finished. Results saved to {output_filepath}")


def run_report_generation():
    """
    Final Reporting Task:
    1.  Reads all the final count files produced by the reducers.
    2.  Aggregates them into a single, final dictionary.
    3.  Sorts the results by frequency in descending order.
    4.  Writes the top 6 results into the final `output.txt` file in a clean, tabular format.
    """
    print("Report Generator: Starting.")
    
    total_counts = {}
    
    reducer_files = glob.glob(os.path.join(INTERMEDIATE_DIR, "reduce_output_*.txt"))

    if not reducer_files:
        sys.exit("Error: No reducer output files found. Did the reduce phase run correctly?")

    for filepath in reducer_files:
        with open(filepath, 'r') as f:
            for line in f:
                try:
                    parts = line.strip().split('\t')
                    if len(parts) == 2:
                        number, count = int(parts[0]), int(parts[1])
                        # Since each reducer handles unique numbers, we can just assign the count.
                        total_counts[number] = count
                except (ValueError, TypeError):
                    continue
                    
    # Sort by the count (the second element of the tuple item) in reverse order.
    sorted_counts = sorted(total_counts.items(), key=lambda item: item[1], reverse=True)
    
    # Write the final formatted report to the specified output file.
    with open(FINAL_OUTPUT_FILE, 'w') as f:
        f.write("--- Top 6 Integers by Frequency ---\n")
        f.write(f"{'Integer':<10}{'Frequency':<10}\n")
        f.write(f"{'-------':<10}{'---------':<10}\n")
        
        for i in range(min(6, len(sorted_counts))):
            number, count = sorted_counts[i]
            f.write(f"{number:<10}{count:<10}\n")
    
    print(f"Report Generator: Finished. Final report is available at {FINAL_OUTPUT_FILE}")


def main():
    """
    Main function to parse command-line arguments and orchestrate the correct task.
    """
    parser = argparse.ArgumentParser(description="A MapReduce pipeline for counting integers.")
    parser.add_argument('phase', choices=['map', 'reduce', 'report'], help="The pipeline phase to execute.")
    parser.add_argument('--id', type=int, help="The task ID for a map or reduce process.")
    args = parser.parse_args()
    
    # The setup should run for any phase to ensure consistency.
    setup_directories()

    if args.phase == 'map':
        if args.id is None:
            sys.exit("Error: The 'map' phase requires a numerical --id.")
        run_mapper(args.id)
    elif args.phase == 'reduce':
        if args.id is None:
            sys.exit("Error: The 'reduce' phase requires a numerical --id.")
        run_reducer(args.id)
    elif args.phase == 'report':
        run_report_generation()
    else:
        # This case should ideally not be reached due to 'choices' in parser.
        sys.exit(f"Error: Invalid phase '{args.phase}' specified.")


if __name__ == "__main__":
    main()